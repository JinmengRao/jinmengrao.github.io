<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Jinmeng Rao</title> <meta name="author" content="Jinmeng Rao"> <meta name="description" content="Jinmeng Rao's personal website. "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%8E&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jinmengrao.github.io/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">News</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/talks/">Talks</a> </li> <li class="nav-item "> <a class="nav-link" href="/services/">Services</a> </li> <li class="nav-item "> <a class="nav-link" href="/awards/">Awards</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Jinmeng Rao </h1> <p class="desc"><span style="color: inherit;font-size:16pt;"><a href="mailto:%6A%69%6E%6D%65%6E%67%72%61%6F@%67%6F%6F%67%6C%65.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=PpZvHlUAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/PhanTask" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/jinmeng-rao-5b4438121" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/JinmengR" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a> <a href="/feed.xml" title="RSS Feed"><i class="fas fa-rss-square"></i></a> </span> | 1600 Amphitheatre Parkway, Mountain View, CA 94043</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prof_pic-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prof_pic-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prof_pic-1400.webp"></source> <img src="/assets/img/prof_pic.jpg?d058db04eae7d74d3271f50f96c66809" class="img-fluid z-depth-1 rounded-circle" width="auto" height="auto" alt="prof_pic.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="clearfix"> <p>I am a research scientist at <a href="https://x.company/projects/mineral/" rel="external nofollow noopener" target="_blank">Mineral, Alphabet Inc</a>. I was a research scientist and AI resident at <a href="https://x.company" rel="external nofollow noopener" target="_blank">Google [X]</a>. I got my PhD in Geographic Information Systems (GIS) and my MS in Computer Sciences (CS) from <a href="https://www.wisc.edu/" rel="external nofollow noopener" target="_blank">University of Wisconsin-Madison</a>, working with <a href="https://geography.wisc.edu/staff/gao-song/" rel="external nofollow noopener" target="_blank">Prof. Song Gao</a> in the <a href="https://geography.wisc.edu/geods/" rel="external nofollow noopener" target="_blank">Geospatial Data Science Lab</a>. Before that, I got my BS and MS in GIS from <a href="https://en.whu.edu.cn/" rel="external nofollow noopener" target="_blank">Wuhan University</a>, working with <a href="https://scholar.google.ca/citations?user=yhy8DgsAAAAJ" rel="external nofollow noopener" target="_blank">Prof. Qingyun Du</a> and <a href="http://sres.whu.edu.cn/info/1238/16237.htm" rel="external nofollow noopener" target="_blank">Prof. Shiliang Su</a>.</p> <p>I work on machine learning, deep learning, geospatial data science, and computational agriculture. In particular, I am interested in developing and incorporating AI approaches into geospatial domains to tackle real-world challenges such as trajectory prediction and generation, location recommendation, spatial data privacy and security, food systems, etc.</p> </div> <h2> <span style="color: inherit;">Recent News</span> <span class="desc" style="font-size:12pt;"><a href="/news/"> [ more... ] </a></span> </h2> <div class="news"> <div class="table-responsive"> <table class="table table-sm table-borderless" cellspacing="0"> <tr> <th scope="row">Oct 2023</th> <td> Our work on multimodal LLM hallucination is accepted for <a href="https://research.google/" rel="external nofollow noopener" target="_blank">Google Research Conference 2023</a>. </td> </tr> <tr> <th scope="row">Oct 2023</th> <td> I served as the PC member of the SoLaR workshop at <a href="https://neurips.cc/" rel="external nofollow noopener" target="_blank">NeurIPS 2023</a>. </td> </tr> <tr> <th scope="row">Oct 2023</th> <td> <a href="https://arxiv.org/abs/2310.00413" rel="external nofollow noopener" target="_blank">Our arXiv preprint</a> on spatial-spectral super-resolution is online. </td> </tr> <tr> <th scope="row">Sep 2023</th> <td> <a href="https://arxiv.org/abs/2309.11587" rel="external nofollow noopener" target="_blank">Our full research paper</a> on conditional trajectory generation is accepted for <a href="https://www.tandfonline.com/journals/tgis20" rel="external nofollow noopener" target="_blank">IJGIS</a>. </td> </tr> <tr> <th scope="row">Sep 2023</th> <td> <a href="https://arxiv.org/abs/2309.17319" rel="external nofollow noopener" target="_blank">Our paper</a> on private&amp;secure GeoAI foundation models is accepted for <a href="https://sigspatial2023.sigspatial.org/" rel="external nofollow noopener" target="_blank">ACM SIGSPATIAL 2023</a>. </td> </tr> <tr> <th scope="row">Sep 2023</th> <td> <a href="https://arxiv.org/abs/2309.04041" rel="external nofollow noopener" target="_blank">Our arXiv preprint</a> on agnosia of multimodal large language models is online. </td> </tr> <tr> <th scope="row">Sep 2023</th> <td> I served as the PC member of the GeoAI workshop at <a href="https://sigspatial2023.sigspatial.org/" rel="external nofollow noopener" target="_blank">ACM SIGSPATIAL 2023</a>. </td> </tr> <tr> <th scope="row">Aug 2023</th> <td> <a href="https://arxiv.org/abs/2308.09970" rel="external nofollow noopener" target="_blank">Our arXiv preprint</a> on vision language reasoning is online and will be presented at <a href="https://baylearn2023.splashthat.com/" rel="external nofollow noopener" target="_blank">BayLearn 2023</a>. </td> </tr> <tr> <th scope="row">May 2023</th> <td> <a href="https://arxiv.org/abs/2305.20047" rel="external nofollow noopener" target="_blank">Our arXiv preprint</a> on open-vocabulary attribute recognition is online. </td> </tr> <tr> <th scope="row">May 2023</th> <td> I served as the co-chair of the GeoIndustry workshop at <a href="https://sigspatial2023.sigspatial.org/" rel="external nofollow noopener" target="_blank">ACM SIGSPATIAL 2023</a>. </td> </tr> </table> </div> </div> <br> <h2> <span style="color: inherit;">Selected Publications</span> <span class="desc" style="font-size:12pt;"><a href="/publications/"> [ more... ] </a></span> </h2> <div class="publications"> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/psgeoai-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/psgeoai-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/psgeoai-1400.webp"></source> <img src="/assets/img/publication_preview/psgeoai.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="psgeoai.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="rao2023psgeoai" class="col-sm-7"> <div class="abbr"><abbr class="badge">ACM SIGSPATIAL</abbr></div> <div class="title">Building Privacy-Preserving and Secure Geospatial Artificial Intelligence Foundation Models</div> <div class="author"> <em>Jinmeng Rao</em>, Song Gao, Gengchen Mai, and Krzysztof Janowicz</div> <div class="periodical"> <em>In Proceedings of the 31st International Conference on Advances in Geographic Information Systems</em>, 2023 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b>Highlight: Top Conference in Geographic Information Systems; Acceptance rate 37.2%</b> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2309.17319" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>In recent years we have seen substantial advances in foundation models for artificial intelligence, including language, vision, and multimodal models. Recent studies have highlighted the potential of using foundation models in geospatial artificial intelligence, known as GeoAI Foundation Models, for geographic question answering, remote sensing image understanding, map generation, and location-based services, among others. However, the development and application of GeoAI foundation models can pose serious privacy and security risks, which have not been fully discussed or addressed to date. This paper introduces the potential privacy and security risks throughout the lifecycle of GeoAI foundation models and proposes a comprehensive blueprint for research directions and preventative and control strategies. Through this vision paper, we hope to draw the attention of researchers and policymakers in geospatial domains to these privacy and security risks inherent in GeoAI foundation models and advocate for the development of privacy-preserving and secure GeoAI foundation models.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">rao2023psgeoai</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Building Privacy-Preserving and Secure Geospatial Artificial Intelligence Foundation Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rao, Jinmeng and Gao, Song and Mai, Gengchen and Janowicz, Krzysztof}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 31st International Conference on Advances in Geographic Information Systems}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--4}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Highlight: Top Conference in Geographic Information Systems; Acceptance rate 37.2\%}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/cats_framework-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/cats_framework-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/cats_framework-1400.webp"></source> <img src="/assets/img/publication_preview/cats_framework.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="cats_framework.jpg" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="rao2023cats" class="col-sm-7"> <div class="abbr"><abbr class="badge">IJGIS</abbr></div> <div class="title">CATS: Conditional Adversarial Trajectory Synthesis for Privacy-Preserving Trajectory Data Publication Using Deep Learning Approaches</div> <div class="author"> <em>Jinmeng Rao</em>, Song Gao, and Sijia Zhu</div> <div class="periodical"> <em>International Journal of Geographical Information Science</em>, 2023 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b>Highlight: Top Journal in Geographic Information Science; SCI Q1 in CS, IS and Geography; Acceptance rate 12%</b> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2309.11587" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/GeoDS/CATS" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>The prevalence of ubiquitous location-aware devices and mobile Internet enables us to collect massive individual-level trajectory dataset from users. Such trajectory big data bring new opportunities to human mobility research but also raise public concerns with regard to location privacy. In this work, we present the Conditional Adversarial Trajectory Synthesis (CATS), a deep-learning-based GeoAI methodological framework for privacy-preserving trajectory data generation and publication. CATS applies K-anonymity to the underlying spatiotemporal distributions of human movements, which provides a distributional-level strong privacy guarantee. By leveraging conditional adversarial training on K-anonymized human mobility matrices, trajectory global context learning using the attention-based mechanism, and recurrent bipartite graph matching of adjacent trajectory points, CATS is able to reconstruct trajectory topology from conditionally sampled locations and generate high-quality individual-level synthetic trajectory data, which can serve as supplements or alternatives to raw data for privacy-preserving trajectory data publication. The experiment results on over 90k GPS trajectories show that our method has a better performance in privacy preservation, spatiotemporal characteristic preservation, and downstream utility compared with baseline methods, which brings new insights into privacy-preserving human mobility research using generative AI techniques and explores data ethics issues in GIScience.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">rao2023cats</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{CATS: Conditional Adversarial Trajectory Synthesis for Privacy-Preserving Trajectory Data Publication Using Deep Learning Approaches}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rao, Jinmeng and Gao, Song and Zhu, Sijia}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Journal of Geographical Information Science}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{37}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{12}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-33}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Highlight: Top Journal in Geographic Information Science; SCI Q1 in CS, IS and Geography; Acceptance rate 12\%}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/emma-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/emma-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/emma-1400.webp"></source> <img src="/assets/img/publication_preview/emma.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="emma.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="lu2023evaluation" class="col-sm-7"> <div class="abbr"><abbr class="badge">ARXIV</abbr></div> <div class="title">Evaluation and Mitigation of Agnosia in Multimodal Large Language Models</div> <div class="author"> <em>Jinmeng Rao*</em>, Jiaying Lu*, Kezhen Chen, Xiaoyuan Guo, Yawen Zhang, Baochen Sun, Carl Yang, and Jie Yang</div> <div class="periodical"> <em>arXiv preprint arXiv:2309.04041</em>, 2023 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b>Highlight: Trustworthy Multimodal Foundation Models; Multimodal Instruction Tuning; The poster version to be presented at Google Research Conference 2023</b> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2309.04041" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>While Multimodal Large Language Models (MLLMs) are widely used for a variety of vision-language tasks, one observation is that they sometimes misinterpret visual inputs or fail to follow textual instructions even in straightforward cases, leading to irrelevant responses, mistakes, and ungrounded claims. This observation is analogous to a phenomenon in neuropsychology known as Agnosia, an inability to correctly process sensory modalities and recognize things (e.g., objects, colors, relations). In our study, we adapt this similar concept to define "agnosia in MLLMs", and our goal is to comprehensively evaluate and mitigate such agnosia in MLLMs. Inspired by the diagnosis and treatment process in neuropsychology, we propose a novel framework EMMA (Evaluation and Mitigation of Multimodal Agnosia). In EMMA, we develop an evaluation module that automatically creates fine-grained and diverse visual question answering examples to assess the extent of agnosia in MLLMs comprehensively. We also develop a mitigation module to reduce agnosia in MLLMs through multimodal instruction tuning on fine-grained conversations. To verify the effectiveness of our framework, we evaluate and analyze agnosia in seven state-of-the-art MLLMs using 9K test samples. The results reveal that most of them exhibit agnosia across various aspects and degrees. We further develop a fine-grained instruction set and tune MLLMs to mitigate agnosia, which led to notable improvement in accuracy.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">lu2023evaluation</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Evaluation and Mitigation of Agnosia in Multimodal Large Language Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rao*, Jinmeng and Lu*, Jiaying and Chen, Kezhen and Guo, Xiaoyuan and Zhang, Yawen and Sun, Baochen and Yang, Carl and Yang, Jie}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2309.04041}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Highlight: Trustworthy Multimodal Foundation Models; Multimodal Instruction Tuning; The poster version to be presented at Google Research Conference 2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/immo-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/immo-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/immo-1400.webp"></source> <img src="/assets/img/publication_preview/immo.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="immo.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="yang2023tackling" class="col-sm-7"> <div class="abbr"><abbr class="badge">ARXIV</abbr></div> <div class="title">Tackling Vision Language Tasks Through Learning Inner Monologues</div> <div class="author"> Diji Yang, Kezhen Chen, <em>Jinmeng Rao</em>, Xiaoyuan Guo, Yawen Zhang, Jie Yang, and Yi Zhang</div> <div class="periodical"> <em>arXiv preprint arXiv:2308.09970</em>, 2023 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b>Highlight: Multi-Agent Collaboration. The poster version was presented at BayLearn 2023</b> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2308.09970" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Visual language tasks require AI models to comprehend and reason with both visual and textual content. Driven by the power of Large Language Models (LLMs), two prominent methods have emerged: (1) the hybrid integration between LLMs and Vision-Language Models (VLMs), where visual inputs are firstly converted into language descriptions by VLMs, serving as inputs for LLMs to generate final answer(s); (2) visual feature alignment in language space, where visual inputs are encoded as embeddings and projected to LLMs’ language space via further supervised fine-tuning. The first approach provides light training costs and interpretability but is hard to be optimized in an end-to-end fashion. The second approach presents decent performance, but feature alignment usually requires large amounts of training data and lacks interpretability. To tackle this dilemma, we propose a novel approach, Inner Monologue Multi-Modal Optimization (IMMO), to solve complex vision language problems by simulating inner monologue processes, a cognitive process in which an individual engages in silent verbal communication with themselves. We enable LLMs and VLMs to interact through natural language conversation and propose to use a two-stage training process to learn how to do the inner monologue (self-asking questions and answering questions). IMMO is evaluated on two popular tasks and the results suggest by emulating the cognitive phenomenon of internal dialogue, our approach can enhance reasoning and explanation abilities, contributing to the more effective fusion of vision and language models. More importantly, instead of using predefined human-crafted monologues, IMMO learns this process within the deep learning models, promising wider applicability to many different AI problems beyond vision language tasks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">yang2023tackling</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Tackling Vision Language Tasks Through Learning Inner Monologues}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yang, Diji and Chen, Kezhen and Rao, Jinmeng and Guo, Xiaoyuan and Zhang, Yawen and Yang, Jie and Zhang, Yi}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2308.09970}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Highlight: Multi-Agent Collaboration. The poster version was presented at BayLearn 2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/geokg-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/geokg-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/geokg-1400.webp"></source> <img src="/assets/img/publication_preview/geokg.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="geokg.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="rao2022measuring" class="col-sm-7"> <div class="abbr"><abbr class="badge">ACM SIGSPATIAL GEOKG</abbr></div> <div class="title">Measuring network resilience via geospatial knowledge graph: a case study of the us multi-commodity flow network</div> <div class="author"> <em>Jinmeng Rao</em>, Song Gao, Michelle Miller, and Alfonso Morales</div> <div class="periodical"> <em>In Proceedings of the 1st ACM SIGSPATIAL International Workshop on Geospatial Knowledge Graphs</em>, 2022 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b>Highlight: Food System; Network Resilience; Geospatial Knowledge Graphs</b> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://dl.acm.org/doi/abs/10.1145/3557990.3567569" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/PhanTask/knowledgegraph" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Quantifying the resilience in the food system is important for food security issues. In this work, we present a geospatial knowledge graph (GeoKG)-based method for measuring the resilience of a multi-commodity flow network. Specifically, we develop a CFS-GeoKG ontology to describe geospatial semantics of a multi-commodity flow network comprehensively, and design resilience metrics that measure the node-level and network-level dependence of single-sourcing, distant, or non-adjacent suppliers/customers in food supply chains. We conduct a case study of the US state-level agricultural multi-commodity flow network with hierarchical commodity types. The results indicate that, by leveraging GeoKG, our method supports measuring both node-level and network-level resilience across space and over time and also helps discover concentration patterns of agricultural resources in the spatial network at different geographic scales.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">rao2022measuring</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Measuring network resilience via geospatial knowledge graph: a case study of the us multi-commodity flow network}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rao, Jinmeng and Gao, Song and Miller, Michelle and Morales, Alfonso}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 1st ACM SIGSPATIAL International Workshop on Geospatial Knowledge Graphs}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{17--25}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Highlight: Food System; Network Resilience; Geospatial Knowledge Graphs}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/stcl-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/stcl-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/stcl-1400.webp"></source> <img src="/assets/img/publication_preview/stcl.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="stcl.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="peng2021spatiotemporal" class="col-sm-7"> <div class="abbr"><abbr class="badge">IEEE IGARSS</abbr></div> <div class="title">Spatiotemporal contrastive representation learning for building damage classification</div> <div class="author"> Bo Peng, Qunying Huang, and <em>Jinmeng Rao</em> </div> <div class="periodical"> <em>In 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS</em>, 2021 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b>Highlight: Spatiotemporal Contrastive Representation Learning; Acceptance rate 34.2%</b> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/9554302" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Automatic building damage assessment after natural disasters is important for emergency response. While existing supervised deep learning models achieved good performance on building damage classification, these models require massive human labels for training. Additionally, pre-trained models often fail to generalize well to new disaster events due to gaps between domains associated with training and testing data. In response, this study proposes a novel spatiotemporal contrastive representation learning model for learning features of building damages with big unlabeled data. Experimental results demonstrate superior performance of such features on classifying building damages resulting from various natural disasters (e.g., hurricanes, floods, wildfires, earthquakes, etc.) across different geographic locations worldwide, compared with the state-of-the-art supervised methods.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">peng2021spatiotemporal</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Spatiotemporal contrastive representation learning for building damage classification}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Peng, Bo and Huang, Qunying and Rao, Jinmeng}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{8562--8565}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Highlight: Spatiotemporal Contrastive Representation Learning; Acceptance rate 34.2\%}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/vtsv.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/vtsv.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/vtsv.gif-1400.webp"></source> <img src="/assets/img/publication_preview/vtsv.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="vtsv.gif" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="rao2021vtsv" class="col-sm-7"> <div class="abbr"><abbr class="badge">ACM SIGSPATIAL GEOAI</abbr></div> <div class="title">VTSV: A privacy-preserving vehicle trajectory simulation and visualization platform using deep reinforcement learning</div> <div class="author"> <em>Jinmeng Rao</em>, Song Gao, and Xiaojin Zhu</div> <div class="periodical"> <em>In Proceedings of the 4th ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery</em>, 2021 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b>Highlight: Deep Reinforcement Learning; Personalized Driving Simulation <a href="https://www.youtube.com/watch?v=NY5L4bu2kTU" rel="external nofollow noopener" target="_blank">[ Video Demo ]</a></b> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://dl.acm.org/doi/10.1145/3486635.3491073" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Trajectory data is among the most sensitive data and the society increasingly raises privacy concerns. In this demo paper, we present a privacy-preserving Vehicle Trajectory Simulation and Visualization (VTSV) web platform (demo video: https://youtu.be/NY5L4bu2kTU), which automatically generates navigation routes between given pairs of origins and destinations and employs a deep reinforcement learning model to simulate vehicle trajectories with customized driving behaviors such as normal driving, overspeed, aggressive acceleration, and aggressive turning. The simulated vehicle trajectory data contain high-sample-rate of attributes including GPS location, speed, acceleration, and steering angle, and such data are visualized in VTSV using streetscape.gl, an autonomous driving data visualization framework. Location privacy protection methods such as origin-destination geomasking and trajectory k-anonymity are integrated into the platform to support privacy-preserving trajectory data generation and publication. We design two application scenarios to demonstrate how VTSV performs location privacy protection and customize driving behavior, respectively. The demonstration shows that VTSV is able to mitigate data privacy, sparsity, and imbalance sampling issues, which offers new insights into driving trajectory simulation and GeoAI-powered privacy-preserving data publication.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">rao2021vtsv</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{VTSV: A privacy-preserving vehicle trajectory simulation and visualization platform using deep reinforcement learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rao, Jinmeng and Gao, Song and Zhu, Xiaojin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 4th ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{43--46}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Highlight: Deep Reinforcement Learning; Personalized Driving Simulation &lt;a href="https://www.youtube.com/watch?v=NY5L4bu2kTU"&gt;[ Video Demo ]&lt;/a&gt;}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/fllocrec-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/fllocrec-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/fllocrec-1400.webp"></source> <img src="/assets/img/publication_preview/fllocrec.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="fllocrec.jpg" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="rao2021privacy" class="col-sm-7"> <div class="abbr"><abbr class="badge">TGIS</abbr></div> <div class="title">A privacy-preserving framework for location recommendation using decentralized collaborative machine learning</div> <div class="author"> <em>Jinmeng Rao</em>, Song Gao, Mingxiao Li, and Qunying Huang</div> <div class="periodical"> <em>Transactions in GIS</em>, 2021 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b>Highlight: Flagship Journal in Geographic Information Systems; Recognized as the first work on Federated Learning + Spatial Data Sciences <a href="https://publications.ait.ac.at/en/publications/on-the-role-of-spatial-data-science-for-federated-learning" rel="external nofollow noopener" target="_blank">(Graser, A. et al 2022)</a></b> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/tgis.12769" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/GeoDS/FL-LocRec" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>The nowadays ubiquitous location-aware mobile devices have contributed to the rapid growth of individual-level location data. Such data are usually collected by location-based service platforms as training data to improve their predictive models’ performance, but the collection of such data may raise public concerns about privacy issues. In this study, we introduce a privacy-preserving location recommendation framework based on a decentralized collaborative machine learning approach: federated learning. Compared with traditional centralized learning frameworks, we keep users’ data on their own devices and train the model locally so that their data remain private. The local model parameters are aggregated and updated through secure multiple-party computation to achieve collaborative learning among users while preserving privacy. Our framework also integrates information about transportation infrastructure, place safety, and flow-based spatial interaction to further improve recommendation accuracy. We further design two attack cases to examine the privacy protection effectiveness and robustness of the framework. The results show that our framework achieves a better balance on the privacy–utility trade-off compared with traditional centralized learning methods. The results and ensuing discussion offer new insights into privacy-preserving geospatial artificial intelligence and promote geoprivacy in location-based services.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">rao2021privacy</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A privacy-preserving framework for location recommendation using decentralized collaborative machine learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rao, Jinmeng and Gao, Song and Li, Mingxiao and Huang, Qunying}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Transactions in GIS}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Highlight: Flagship Journal in Geographic Information Systems; Recognized as the first work on Federated Learning + Spatial Data Sciences &lt;a href="https://publications.ait.ac.at/en/publications/on-the-role-of-spatial-data-science-for-federated-learning"&gt;(Graser, A. et al 2022)&lt;/a&gt;}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/trajgan-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/trajgan-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/trajgan-1400.webp"></source> <img src="/assets/img/publication_preview/trajgan.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="trajgan.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="rao2020lstm" class="col-sm-7"> <div class="abbr"><abbr class="badge">GISCIENCE</abbr></div> <div class="title">LSTM-TrajGAN: A deep learning approach to trajectory privacy protection</div> <div class="author"> <em>Jinmeng Rao</em>, Song Gao, Yuhao Kang, and Qunying Huang</div> <div class="periodical"> <em>In 11th International Conference on Geographic Information Science (GIScience 2021) - Part I</em>, 2020 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b>Highlight: Top Conference in Geographic Information Sciences; Acceptance rate 34%</b> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://drops.dagstuhl.de/opus/volltexte/2020/13047/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/GeoDS/LSTM-TrajGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>The prevalence of location-based services contributes to the explosive growth of individual-level location trajectory data and raises public concerns about privacy issues. In this research, we propose a novel LSTM-TrajGAN approach, which is an end-to-end deep learning model to generate privacy-preserving synthetic trajectory data for data sharing and publication. We design a loss metric function TrajLoss to measure the trajectory similarity losses for model training and optimization. The model is evaluated on the trajectory-user-linking task on a real-world semantic trajectory dataset. Compared with other common geomasking methods, our model can better prevent users from being re-identified, and it also preserves essential spatial, temporal, and thematic characteristics of the real trajectory data. The model better balances the effectiveness of trajectory privacy protection and the utility for spatial and temporal analyses, which offers new insights into the GeoAI-powered privacy protection for human mobility studies.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">rao2020lstm</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{LSTM-TrajGAN: A deep learning approach to trajectory privacy protection}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rao, Jinmeng and Gao, Song and Kang, Yuhao and Huang, Qunying}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{11th International Conference on Geographic Information Science (GIScience 2021) - Part I}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{177}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{12--1}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{Schloss Dagstuhl--Leibniz-Zentrum f$\{$$\backslash$"u$\}$r Informatik}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Highlight: Top Conference in Geographic Information Sciences; Acceptance rate 34\%}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/mobmap.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/mobmap.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/mobmap.gif-1400.webp"></source> <img src="/assets/img/publication_preview/mobmap.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="mobmap.gif" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="gao2020mapping" class="col-sm-7"> <div class="abbr"><abbr class="badge">ACM SIGSPATIAL</abbr></div> <div class="title">Mapping county-level mobility pattern changes in the United States in response to COVID-19</div> <div class="author"> Song Gao, <em>Jinmeng Rao</em>, Yuhao Kang, Yunlei Liang, and Jake Kruse</div> <div class="periodical"> <em>SIGSpatial Special</em>, 2020 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b>Highlight: The first work on social distancing mapping during COVID-19; Media Coverage <a href="https://www.nbc15.com/2020/10/07/tracking-movement-in-wisconsin-leading-up-to-latest-health-order/" rel="external nofollow noopener" target="_blank">NBC</a>, <a href="http://fox47.com/news/local/uw-madison-researchers-map-travel-data-to-combat-pandemic" rel="external nofollow noopener" target="_blank">FOX</a>, <a href="https://www.wisn.com/article/uw-madison-scientists-develop-social-distancing-tracker-map/32163419#" rel="external nofollow noopener" target="_blank">ABC</a></b> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://dl.acm.org/doi/10.1145/3404820.3404824" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/GeoDS/mapmobility" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://geography.wisc.edu/covid19/physical-distancing/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>To contain the COVID-19 epidemic, one of the non-pharmacological epidemic control measures is reducing the transmission rate of SARS-COV-2 in the population through social distancing. An interactive web-based mapping platform that provides timely quantitative information on how people in different counties and states reacted to the social distancing guidelines was developed by the GeoDS Lab @UW-Madison with the support of the National Science Foundation RAPID program. The web portal integrates geographic information systems (GIS) and daily updated human mobility statistical patterns (median travel distance and stay-at-home dwell time) derived from large-scale anonymized and aggregated smartphone location big data at the county-level in the United States, and aims to increase risk awareness of the public, support data-driven public health and governmental decision-making, and help enhance community responses to the COVID-19 pandemic.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">gao2020mapping</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Mapping county-level mobility pattern changes in the United States in response to COVID-19}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao, Song and Rao, Jinmeng and Kang, Yuhao and Liang, Yunlei and Kruse, Jake}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{SIGSpatial Special}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{12}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{16--26}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{ACM New York, NY, USA}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Highlight: The first work on social distancing mapping during COVID-19; Media Coverage &lt;a href="https://www.nbc15.com/2020/10/07/tracking-movement-in-wisconsin-leading-up-to-latest-health-order/"&gt;NBC&lt;/a&gt;, &lt;a href="http://fox47.com/news/local/uw-madison-researchers-map-travel-data-to-combat-pandemic"&gt;FOX&lt;/a&gt;, &lt;a href="https://www.wisn.com/article/uw-madison-scientists-develop-social-distancing-tracker-map/32163419#"&gt;ABC&lt;/a&gt;}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <br> <h2> <span style="color: inherit;">Invited Talks</span> <span class="desc" style="font-size:12pt;"><a href="/talks/"> [ more... ] </a></span> </h2> <div class="news"> <div class="table-responsive"> <table class="table table-sm table-borderless" cellspacing="0"> <tr> <th scope="row">2023</th> <td> Privacy and Security Issues in Large-Language-Model-powered Geospatial Applications. Applications of ChatGPT / AI at Geography According to ChatGPT, University of California, Santa Barbara <a href="https://www.airmeet.com/e/c4516530-ce69-11ed-a388-d572e3feeb08" rel="external nofollow noopener" target="_blank">[ link ]</a> </td> </tr> <tr> <th scope="row">2023</th> <td> Mapping Mobility Changes During COVID-19 Using ArcGIS Dashboards and Mobile Experience Builder, Esri Education Sessions and Activities at AAG 2023 <a href="https://community.esri.com/t5/higher-education-docs/esri-education-sessions-and-activities-at-aag-2023/ta-p/1265486/jump-to/first-unread-message" rel="external nofollow noopener" target="_blank">[ link ]</a> </td> </tr> <tr> <th scope="row">2023</th> <td> Privacy-Preserving Location Recommendation via Decentralized Collaborative Learning. Texas A&amp;M Built Environment Talk Series <a href="https://urbanai.tamids.tamu.edu/2023/04/09/built-env-talk-series-dr-jinmeng-rao/" rel="external nofollow noopener" target="_blank">[ link ]</a> </td> </tr> <tr> <th scope="row">2023</th> <td> Deep Reinforcement Learning for Driving Trajectory Simulation. Reinforcement Learning Seminar, Anhui University </td> </tr> <tr> <th scope="row">2023</th> <td> Geographic Visualization and ArcGIS Dashboards, Geovisualization and Spatial Analysis Seminar, University of Arkansas </td> </tr> <tr> <th scope="row">2022</th> <td> Trajectory Privacy Protection with Geospatial Artificial Intelligence, UW Roundtable on the Frontiers of Geospatial Artificial Intelligence, University of Washington <a href="https://hgis.uw.edu/2022/10/28/uw-roundtable-on-the-frontiers-of-geospatial-artificial-intelligence/" rel="external nofollow noopener" target="_blank">[ link ]</a> </td> </tr> </table> </div> </div> <br> <h2> <span style="color: inherit;">Recent Services</span> <span class="desc" style="font-size:12pt;"><a href="/services/"> [ more... ] </a></span> </h2> <div class="news"> <div class="table-responsive"> <p><span class="desc" style="font-size:14pt;">Program Organizers</span></p> <table class="table table-sm table-borderless" cellspacing="0"> <tr> <th scope="row">2023</th> <td> <strong>PC Member</strong>, NeurIPS 2023 Workshop on Socially Responsible Language Modelling Research (SoLaR 2023) </td> </tr> <tr> <th scope="row">2023</th> <td> <strong>PC Member</strong>, ACM SIGSPATIAL 2023 International Workshop on on AI for Geographic Knowledge Discovery (GeoAI 2023) </td> </tr> <tr> <th scope="row">2023</th> <td> <strong>Chair</strong>, ACM SIGSPATIAL 2023 International Workshop on Spatial Big Data and AI for Industrial Applications (GeoIndustry 2023) </td> </tr> <tr> <th scope="row">2023</th> <td> <strong>Organizer</strong>, AAG 2023 GeoAI and Deep Learning Symposium: Geoprivacy and Ethics in Geospatial Data and GeoAI </td> </tr> <tr> <th scope="row">2022</th> <td> <strong>Chair</strong>, AAG 2022 Symposium on Data-Intensive Geospatial Understanding in the Era of AI and CyberGIS: GeoAI for Social Sensing </td> </tr> </table> </div> <div class="table-responsive"> <p><span class="desc" style="font-size:14pt;">Reviewers</span></p> <ul> <li>ACM SIGSPATIAL</li> <li>ACM Transactions on Spatial Algorithms and Systems</li> <li>Annals of GIS</li> <li>Applied Computing and Geosciences</li> <li>Applied Sciences</li> <li>BMC Public Health</li> <li>Computers &amp; Geosciences</li> <li>Data &amp; Policy</li> <li>Earth Science Informatics</li> <li>Habitat International</li> <li>International Conference on Geoinformatics</li> <li>International Journal of Data Science and Analytics</li> <li>International Journal of Geographical Information Science</li> <li>International Journal of Health Geographics</li> <li>ISPRS International Journal of Geo-Information</li> <li>Journal of Computational Social Science</li> <li>Journal of Geovisualization and Spatial Analysis</li> <li>Journal of Maps</li> <li>Land Use Policy</li> <li>NeurIPS Workshops</li> <li>Scientific Reports</li> <li>Spatial and Spatio-temporal Epidemiology</li> <li>Stochastic Environmental Research and Risk Assessment</li> <li>Transactions in GIS</li> </ul> </div> <div class="table-responsive"> <p><span class="desc" style="font-size:14pt;">Professional Appointments</span></p> <table class="table table-sm table-borderless" cellspacing="0"> <tr> <th scope="row">2022 - 2023</th> <td> <strong>Committee Member</strong>, Career Development Committee, Chinese Professionals in Geographic Information Sciences (CPGIS) </td> </tr> <tr> <th scope="row">2022 - 2023</th> <td> <strong>Committee Member</strong>, Media Committee, Chinese Professionals in Geographic Information Sciences (CPGIS) </td> </tr> <tr> <th scope="row">2019 - 2023</th> <td> <strong>Co-Founder and Director</strong>, GISphere, a non-profit org promoting GIS-related education </td> </tr> <tr> <th scope="row">2021 - 2022</th> <td> <strong>Student Director</strong>, American Association of Geographers Geographic Information Science and Systems Specialty Group (AAG-GISS) </td> </tr> <tr> <th scope="row">2021 - 2022</th> <td> <strong>Committee Member</strong>, Graduate Study Committee, Department of Geography, University of Wisconsin-Madison </td> </tr> <tr> <th scope="row">2020 - 2021</th> <td> <strong>Committee Member</strong>, Curriculum Committee, Department of Geography, University of Wisconsin-Madison </td> </tr> <tr> <th scope="row">2017 - 2018</th> <td> <strong>Network Center Assistant</strong>, School of Resource and Environmental Sciences, Wuhan University </td> </tr> </table> </div> </div> <br> <h2> <span style="color: inherit;">Recent Awards</span> <span class="desc" style="font-size:12pt;"><a href="/awards/"> [ more... ] </a></span> </h2> <div class="news"> <div class="table-responsive"> <table class="table table-sm table-borderless" cellspacing="0"> <tr> <th scope="row">2023</th> <td> Peer Bonus / Spot Bonus, Google [X] / Mineral, Alphabet Inc. </td> </tr> <tr> <th scope="row">2022</th> <td> The SRGC Conference Presentation Award, University of Wisconsin-Madison </td> </tr> <tr> <th scope="row">2022</th> <td> Trewartha/Odebolt Conference Travel Award, University of Wisconsin-Madison </td> </tr> <tr> <th scope="row">2019</th> <td> Best Poster Paper, ACM SIGSPATIAL RAAS Workshop </td> </tr> <tr> <th scope="row">2019</th> <td> Welcome Award, Graduate School, University of Wisconsin-Madison </td> </tr> <tr> <th scope="row">2018</th> <td> Outstanding Graduate Student, Wuhan University </td> </tr> <tr> <th scope="row">2018</th> <td> Silver Award, China “Internet+” Innovation and Entrepreneurship Competition Hubei Final </td> </tr> <tr> <th scope="row">2017</th> <td> The 1st Prize, Wuhan University “Internet+” Innovation and Entrepreneurship Competition </td> </tr> <tr> <th scope="row">2017</th> <td> National Scholarship, Ministry of Education, China </td> </tr> <tr> <th scope="row">2015</th> <td> National Scholarship, Ministry of Education, China </td> </tr> </table> </div> </div> <br> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2023 Jinmeng Rao. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?99f5dd6bf47fe01169a4267f279df989"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>