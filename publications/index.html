<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Publications | Jinmeng Rao</title> <meta name="author" content="Jinmeng Rao"> <meta name="description" content="Jinmeng Rao's personal website. "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%8E&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jinmengrao.github.io/publications/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Jinmeng Rao</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">News</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/talks/">Talks</a> </li> <li class="nav-item "> <a class="nav-link" href="/services/">Services</a> </li> <li class="nav-item "> <a class="nav-link" href="/awards/">Awards</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description"></p> </header> <article> <div class="publications"> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/ssif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/ssif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/ssif-1400.webp"></source> <img src="/assets/img/publication_preview/ssif.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="ssif.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="mai2023ssif" class="col-sm-7"> <div class="abbr"><abbr class="badge">ARXIV</abbr></div> <div class="title">SSIF: Learning Continuous Image Representation for Spatial-Spectral Super-Resolution</div> <div class="author"> Gengchen Mai, Ni Lao, Weiwei Sun, Yuchi Ma, Jiaming Song, Chenlin Meng, Hongxu Ma, <em>Jinmeng Rao</em>, Ziyuan Li, and Stefano Ermon</div> <div class="periodical"> <em>arXiv preprint arXiv:2310.00413</em>, 2023 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b></b> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2310.00413" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-arxiv-id="2310.00413"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Existing digital sensors capture images at fixed spatial and spectral resolutions (e.g., RGB, multispectral, and hyperspectral images), and each combination requires bespoke machine learning models. Neural Implicit Functions partially overcome the spatial resolution challenge by representing an image in a resolution-independent way. However, they still operate at fixed, pre-defined spectral resolutions. To address this challenge, we propose Spatial-Spectral Implicit Function (SSIF), a neural implicit model that represents an image as a function of both continuous pixel coordinates in the spatial domain and continuous wavelengths in the spectral domain. We empirically demonstrate the effectiveness of SSIF on two challenging spatio-spectral super-resolution benchmarks. We observe that SSIF consistently outperforms state-of-the-art baselines even when the baselines are allowed to train separate models at each spectral resolution. We show that SSIF generalizes well to both unseen spatial resolutions and spectral resolutions. Moreover, SSIF can generate high-resolution images that improve the performance of downstream tasks (e.g., land use classification) by 1.7%-7%.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">mai2023ssif</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SSIF: Learning Continuous Image Representation for Spatial-Spectral Super-Resolution}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mai, Gengchen and Lao, Ni and Sun, Weiwei and Ma, Yuchi and Song, Jiaming and Meng, Chenlin and Ma, Hongxu and Rao, Jinmeng and Li, Ziyuan and Ermon, Stefano}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2310.00413}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2310.00413}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CV}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/psgeoai-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/psgeoai-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/psgeoai-1400.webp"></source> <img src="/assets/img/publication_preview/psgeoai.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="psgeoai.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="rao2023psgeoai" class="col-sm-7"> <div class="abbr"><abbr class="badge">ACM SIGSPATIAL</abbr></div> <div class="title">Building Privacy-Preserving and Secure Geospatial Artificial Intelligence Foundation Models</div> <div class="author"> <em>Jinmeng Rao</em>, Song Gao, Gengchen Mai, and Krzysztof Janowicz</div> <div class="periodical"> <em>In Proceedings of the 31st International Conference on Advances in Geographic Information Systems</em>, 2023 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b>Highlight: Top Conference in Geographic Information Systems; Acceptance rate 37.2%</b> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2309.17319" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>In recent years we have seen substantial advances in foundation models for artificial intelligence, including language, vision, and multimodal models. Recent studies have highlighted the potential of using foundation models in geospatial artificial intelligence, known as GeoAI Foundation Models, for geographic question answering, remote sensing image understanding, map generation, and location-based services, among others. However, the development and application of GeoAI foundation models can pose serious privacy and security risks, which have not been fully discussed or addressed to date. This paper introduces the potential privacy and security risks throughout the lifecycle of GeoAI foundation models and proposes a comprehensive blueprint for research directions and preventative and control strategies. Through this vision paper, we hope to draw the attention of researchers and policymakers in geospatial domains to these privacy and security risks inherent in GeoAI foundation models and advocate for the development of privacy-preserving and secure GeoAI foundation models.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">rao2023psgeoai</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Building Privacy-Preserving and Secure Geospatial Artificial Intelligence Foundation Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rao, Jinmeng and Gao, Song and Mai, Gengchen and Janowicz, Krzysztof}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 31st International Conference on Advances in Geographic Information Systems}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--4}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Highlight: Top Conference in Geographic Information Systems; Acceptance rate 37.2\%}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/cats_framework-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/cats_framework-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/cats_framework-1400.webp"></source> <img src="/assets/img/publication_preview/cats_framework.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="cats_framework.jpg" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="rao2023cats" class="col-sm-7"> <div class="abbr"><abbr class="badge">IJGIS</abbr></div> <div class="title">CATS: Conditional Adversarial Trajectory Synthesis for Privacy-Preserving Trajectory Data Publication Using Deep Learning Approaches</div> <div class="author"> <em>Jinmeng Rao</em>, Song Gao, and Sijia Zhu</div> <div class="periodical"> <em>International Journal of Geographical Information Science</em>, 2023 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b>Highlight: Top Journal in Geographic Information Science; SCI Q1 in CS, IS and Geography; Acceptance rate 12%</b> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2309.11587" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/GeoDS/CATS" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>The prevalence of ubiquitous location-aware devices and mobile Internet enables us to collect massive individual-level trajectory dataset from users. Such trajectory big data bring new opportunities to human mobility research but also raise public concerns with regard to location privacy. In this work, we present the Conditional Adversarial Trajectory Synthesis (CATS), a deep-learning-based GeoAI methodological framework for privacy-preserving trajectory data generation and publication. CATS applies K-anonymity to the underlying spatiotemporal distributions of human movements, which provides a distributional-level strong privacy guarantee. By leveraging conditional adversarial training on K-anonymized human mobility matrices, trajectory global context learning using the attention-based mechanism, and recurrent bipartite graph matching of adjacent trajectory points, CATS is able to reconstruct trajectory topology from conditionally sampled locations and generate high-quality individual-level synthetic trajectory data, which can serve as supplements or alternatives to raw data for privacy-preserving trajectory data publication. The experiment results on over 90k GPS trajectories show that our method has a better performance in privacy preservation, spatiotemporal characteristic preservation, and downstream utility compared with baseline methods, which brings new insights into privacy-preserving human mobility research using generative AI techniques and explores data ethics issues in GIScience.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">rao2023cats</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{CATS: Conditional Adversarial Trajectory Synthesis for Privacy-Preserving Trajectory Data Publication Using Deep Learning Approaches}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rao, Jinmeng and Gao, Song and Zhu, Sijia}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Journal of Geographical Information Science}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{37}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{12}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-33}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Highlight: Top Journal in Geographic Information Science; SCI Q1 in CS, IS and Geography; Acceptance rate 12\%}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/emma-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/emma-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/emma-1400.webp"></source> <img src="/assets/img/publication_preview/emma.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="emma.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="lu2023evaluation" class="col-sm-7"> <div class="abbr"><abbr class="badge">ARXIV</abbr></div> <div class="title">Evaluation and Mitigation of Agnosia in Multimodal Large Language Models</div> <div class="author"> <em>Jinmeng Rao*</em>, Jiaying Lu*, Kezhen Chen, Xiaoyuan Guo, Yawen Zhang, Baochen Sun, Carl Yang, and Jie Yang</div> <div class="periodical"> <em>arXiv preprint arXiv:2309.04041</em>, 2023 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b>Highlight: Trustworthy Multimodal Foundation Models; Multimodal Instruction Tuning</b> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2309.04041" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>While Multimodal Large Language Models (MLLMs) are widely used for a variety of vision-language tasks, one observation is that they sometimes misinterpret visual inputs or fail to follow textual instructions even in straightforward cases, leading to irrelevant responses, mistakes, and ungrounded claims. This observation is analogous to a phenomenon in neuropsychology known as Agnosia, an inability to correctly process sensory modalities and recognize things (e.g., objects, colors, relations). In our study, we adapt this similar concept to define "agnosia in MLLMs", and our goal is to comprehensively evaluate and mitigate such agnosia in MLLMs. Inspired by the diagnosis and treatment process in neuropsychology, we propose a novel framework EMMA (Evaluation and Mitigation of Multimodal Agnosia). In EMMA, we develop an evaluation module that automatically creates fine-grained and diverse visual question answering examples to assess the extent of agnosia in MLLMs comprehensively. We also develop a mitigation module to reduce agnosia in MLLMs through multimodal instruction tuning on fine-grained conversations. To verify the effectiveness of our framework, we evaluate and analyze agnosia in seven state-of-the-art MLLMs using 9K test samples. The results reveal that most of them exhibit agnosia across various aspects and degrees. We further develop a fine-grained instruction set and tune MLLMs to mitigate agnosia, which led to notable improvement in accuracy.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">lu2023evaluation</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Evaluation and Mitigation of Agnosia in Multimodal Large Language Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rao*, Jinmeng and Lu*, Jiaying and Chen, Kezhen and Guo, Xiaoyuan and Zhang, Yawen and Sun, Baochen and Yang, Carl and Yang, Jie}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2309.04041}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Highlight: Trustworthy Multimodal Foundation Models; Multimodal Instruction Tuning}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/ikle-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/ikle-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/ikle-1400.webp"></source> <img src="/assets/img/publication_preview/ikle.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="ikle.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="tu2023interactive" class="col-sm-7"> <div class="abbr"><abbr class="badge">IEEE CG&amp;A</abbr></div> <div class="title">An Interactive Knowledge and Learning Environment in Smart Foodsheds</div> <div class="author"> Yamei Tu, Xiaoqi Wang, Rui Qiu, Han-Wei Shen, Michelle Miller, <em>Jinmeng Rao</em>, Song Gao, Patrick R Huber, Allan D Hollander, Matthew Lange, and  others</div> <div class="periodical"> <em>IEEE Computer Graphics and Applications</em>, 2023 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b></b> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/10091124" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>The Internet of Food (IoF) is an emerging field in smart foodsheds, involving the creation of a knowledge graph (KG) about the environment, agriculture, food, diet, and health. However, the heterogeneity and size of the KG present challenges for downstream tasks, such as information retrieval and interactive exploration. To address those challenges, we propose an interactive knowledge and learning environment (IKLE) that integrates three programming and modeling languages to support multiple downstream tasks in the analysis pipeline. To make IKLE easier to use, we have developed algorithms to automate the generation of each language. In addition, we collaborated with domain experts to design and develop a dataflow visualization system, which embeds the automatic language generations into components and allows users to build their analysis pipeline by dragging and connecting components of interest. We have demonstrated the effectiveness of IKLE through three real-world case studies in smart foodsheds.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tu2023interactive</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{An Interactive Knowledge and Learning Environment in Smart Foodsheds}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tu, Yamei and Wang, Xiaoqi and Qiu, Rui and Shen, Han-Wei and Miller, Michelle and Rao, Jinmeng and Gao, Song and Huber, Patrick R and Hollander, Allan D and Lange, Matthew and others}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Computer Graphics and Applications}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/lowa-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/lowa-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/lowa-1400.webp"></source> <img src="/assets/img/publication_preview/lowa.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="lowa.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="guo2023lowa" class="col-sm-7"> <div class="abbr"><abbr class="badge">ARXIV</abbr></div> <div class="title">LOWA: Localize Objects in the Wild with Attributes</div> <div class="author"> Xiaoyuan Guo, Kezhen Chen, <em>Jinmeng Rao</em>, Yawen Zhang, Baochen Sun, and Jie Yang</div> <div class="periodical"> <em>arXiv preprint arXiv:2305.20047</em>, 2023 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b></b> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2305.20047" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>We present LOWA, a novel method for localizing objects with attributes effectively in the wild. It aims to address the insufficiency of current open-vocabulary object detectors, which are limited by the lack of instance-level attribute classification and rare class names. To train LOWA, we propose a hybrid vision-language training strategy to learn object detection and recognition with class names as well as attribute information. With LOWA, users can not only detect objects with class names, but also able to localize objects by attributes. LOWA is built on top of a two-tower vision-language architecture and consists of a standard vision transformer as the image encoder and a similar transformer as the text encoder. To learn the alignment between visual and text inputs at the instance level, we train LOWA with three training steps: object-level training, attribute-aware learning, and free-text joint training of objects and attributes. This hybrid training strategy first ensures correct object detection, then incorporates instance-level attribute information, and finally balances the object class and attribute sensitivity. We evaluate our model performance of attribute classification and attribute localization on the Open-Vocabulary Attribute Detection (OVAD) benchmark and the Visual Attributes in the Wild (VAW) dataset, and experiments indicate strong zero-shot performance. Ablation studies additionally demonstrate the effectiveness of each training step of our approach.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">guo2023lowa</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{LOWA: Localize Objects in the Wild with Attributes}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Guo, Xiaoyuan and Chen, Kezhen and Rao, Jinmeng and Zhang, Yawen and Sun, Baochen and Yang, Jie}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2305.20047}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/platial-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/platial-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/platial-1400.webp"></source> <img src="/assets/img/publication_preview/platial.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="platial.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="liuhere" class="col-sm-7"> <div class="abbr"><abbr class="badge">PLATIAL</abbr></div> <div class="title">Here is Not There: Measuring Entailment-based Trajectory Similarity for Location-Privacy Protection and Beyond</div> <div class="author"> Zilong Liu, Krzysztof Janowicz, Kitty Currier, Meilin Shi, <em>Jinmeng Rao</em>, Song Gao, Ling Cai, and Anita Graser</div> <div class="periodical"> <em>In International Symposium on Platial Information Science</em>, 2023 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b></b> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://zenodo.org/record/8286277" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>While the paths humans take play out in social as well as physical space, measures to describe and compare their trajectories are carried out in abstract, typically Euclidean, space. When these measures are applied to trajectories of actual individuals in an application area, alterations that are inconsequential in abstract space may suddenly become problematic once overlaid with geographical reality. In this work, we present a different view on trajectory similarity by introducing a measure that utilizes logical entailment. This is an inferential perspective that considers facts as triple statements deduced from the social and environmental context, in which the travel takes place, and their practical implications. We suggest a formalization of entailment-based trajectory similarity, measured as the overlapping proportion of facts, which are spatial relation statements in our case study. With the proposed measure, we evaluate LSTM-TrajGAN, a privacy-preserving trajectory-generation model. The entailment-based model evaluation reveals potential consequences of disregarding the rich structure of geographical space (e.g., miscalculated insurance risk due to regional shifts in our toy example). Our work highlights the advantage of applying logical entailment to trajectory-similarity reasoning for location-privacy protection and beyond.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">liuhere</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Here is Not There: Measuring Entailment-based Trajectory Similarity for Location-Privacy Protection and Beyond}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Zilong and Janowicz, Krzysztof and Currier, Kitty and Shi, Meilin and Rao, Jinmeng and Gao, Song and Cai, Ling and Graser, Anita}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Symposium on Platial Information Science}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/immo-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/immo-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/immo-1400.webp"></source> <img src="/assets/img/publication_preview/immo.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="immo.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="yang2023tackling" class="col-sm-7"> <div class="abbr"><abbr class="badge">ARXIV</abbr></div> <div class="title">Tackling Vision Language Tasks Through Learning Inner Monologues</div> <div class="author"> Diji Yang, Kezhen Chen, <em>Jinmeng Rao</em>, Xiaoyuan Guo, Yawen Zhang, Jie Yang, and Yi Zhang</div> <div class="periodical"> <em>arXiv preprint arXiv:2308.09970</em>, 2023 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b>Highlight: Multi-Agent Collaboration. To be presented at BayLearn 2023</b> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2308.09970" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Visual language tasks require AI models to comprehend and reason with both visual and textual content. Driven by the power of Large Language Models (LLMs), two prominent methods have emerged: (1) the hybrid integration between LLMs and Vision-Language Models (VLMs), where visual inputs are firstly converted into language descriptions by VLMs, serving as inputs for LLMs to generate final answer(s); (2) visual feature alignment in language space, where visual inputs are encoded as embeddings and projected to LLMs’ language space via further supervised fine-tuning. The first approach provides light training costs and interpretability but is hard to be optimized in an end-to-end fashion. The second approach presents decent performance, but feature alignment usually requires large amounts of training data and lacks interpretability. To tackle this dilemma, we propose a novel approach, Inner Monologue Multi-Modal Optimization (IMMO), to solve complex vision language problems by simulating inner monologue processes, a cognitive process in which an individual engages in silent verbal communication with themselves. We enable LLMs and VLMs to interact through natural language conversation and propose to use a two-stage training process to learn how to do the inner monologue (self-asking questions and answering questions). IMMO is evaluated on two popular tasks and the results suggest by emulating the cognitive phenomenon of internal dialogue, our approach can enhance reasoning and explanation abilities, contributing to the more effective fusion of vision and language models. More importantly, instead of using predefined human-crafted monologues, IMMO learns this process within the deep learning models, promising wider applicability to many different AI problems beyond vision language tasks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">yang2023tackling</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Tackling Vision Language Tasks Through Learning Inner Monologues}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yang, Diji and Chen, Kezhen and Rao, Jinmeng and Guo, Xiaoyuan and Zhang, Yawen and Yang, Jie and Zhang, Yi}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2308.09970}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Highlight: Multi-Agent Collaboration. To be presented at BayLearn 2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/tgis_edu-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/tgis_edu-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/tgis_edu-1400.webp"></source> <img src="/assets/img/publication_preview/tgis_edu.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="tgis_edu.jpg" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="wang2023choosing" class="col-sm-7"> <div class="abbr"><abbr class="badge">TGIS</abbr></div> <div class="title">Choosing GIS graduate programs from afar: Chinese students’ perspectives</div> <div class="author"> Yikang Wang, Yuhao Kang, Haokun Liu, Ce Hou, Bing Zhou, Shan Ye, Yuyan Liu, <em>Jinmeng Rao</em>, Zhenghao Pei, Xiang Ye, and  others</div> <div class="periodical"> <em>Transactions in GIS</em>, 2023 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b></b> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/tgis.13037" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>With the increasing demands for geospatial analytics in industry and academia, the need for Geographic Information Systems/Science (GIS) education is on the rise. A growing number of departments in geography have launched or expanded their GIS graduate programs. However, the factors influencing students choosing GIS programs have not been examined yet. In this study, we looked at Chinese students applying for overseas GIS programs and examined factors influencing their decision-making. We distributed the survey in GISphere, one of the largest GIS international student communities, and 84 valid questionnaires were returned. We analyzed the spatial and demographic patterns of Chinese students applying for overseas GIS programs. We identify five main factors that affect their choices of GIS programs: (1) education quality and awareness, (2) physical, social, and political environments, (3) improved employment prospects, (4) personal recommendations, and (5) the application process. Our study offers implications for the development of GIS graduate programs. We anticipate that the conclusions drawn from this research will benefit and advance geography and GIS education globally.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">wang2023choosing</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Choosing GIS graduate programs from afar: Chinese students' perspectives}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang, Yikang and Kang, Yuhao and Liu, Haokun and Hou, Ce and Zhou, Bing and Ye, Shan and Liu, Yuyan and Rao, Jinmeng and Pei, Zhenghao and Ye, Xiang and others}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Transactions in GIS}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{27}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{450--475}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/geokg-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/geokg-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/geokg-1400.webp"></source> <img src="/assets/img/publication_preview/geokg.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="geokg.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="rao2022measuring" class="col-sm-7"> <div class="abbr"><abbr class="badge">ACM SIGSPATIAL GEOKG</abbr></div> <div class="title">Measuring network resilience via geospatial knowledge graph: a case study of the us multi-commodity flow network</div> <div class="author"> <em>Jinmeng Rao</em>, Song Gao, Michelle Miller, and Alfonso Morales</div> <div class="periodical"> <em>In Proceedings of the 1st ACM SIGSPATIAL International Workshop on Geospatial Knowledge Graphs</em>, 2022 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b>Highlight: Food System; Network Resilience; Geospatial Knowledge Graphs</b> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://dl.acm.org/doi/abs/10.1145/3557990.3567569" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/PhanTask/knowledgegraph" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Quantifying the resilience in the food system is important for food security issues. In this work, we present a geospatial knowledge graph (GeoKG)-based method for measuring the resilience of a multi-commodity flow network. Specifically, we develop a CFS-GeoKG ontology to describe geospatial semantics of a multi-commodity flow network comprehensively, and design resilience metrics that measure the node-level and network-level dependence of single-sourcing, distant, or non-adjacent suppliers/customers in food supply chains. We conduct a case study of the US state-level agricultural multi-commodity flow network with hierarchical commodity types. The results indicate that, by leveraging GeoKG, our method supports measuring both node-level and network-level resilience across space and over time and also helps discover concentration patterns of agricultural resources in the spatial network at different geographic scales.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">rao2022measuring</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Measuring network resilience via geospatial knowledge graph: a case study of the us multi-commodity flow network}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rao, Jinmeng and Gao, Song and Miller, Michelle and Morales, Alfonso}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 1st ACM SIGSPATIAL International Workshop on Geospatial Knowledge Graphs}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{17--25}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Highlight: Food System; Network Resilience; Geospatial Knowledge Graphs}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/sticc-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/sticc-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/sticc-1400.webp"></source> <img src="/assets/img/publication_preview/sticc.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="sticc.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="kang2022sticc" class="col-sm-7"> <div class="abbr"><abbr class="badge">IJGIS</abbr></div> <div class="title">STICC: a multivariate spatial clustering method for repeated geographic pattern discovery with consideration of spatial contiguity</div> <div class="author"> Yuhao Kang, Kunlin Wu, Song Gao, Ignavier Ng, <em>Jinmeng Rao</em>, Shan Ye, Fan Zhang, and Teng Fei</div> <div class="periodical"> <em>International Journal of Geographical Information Science</em>, 2022 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b></b> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.tandfonline.com/doi/abs/10.1080/13658816.2022.2053980" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/GeoDS/STICC" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Spatial clustering has been widely used for spatial data mining and knowledge discovery. An ideal multivariate spatial clustering should consider both spatial contiguity and aspatial attributes. Existing spatial clustering approaches may face challenges for discovering repeated geographic patterns with spatial contiguity maintained. In this paper, we propose a Spatial Toeplitz Inverse Covariance-Based Clustering (STICC) method that considers both attributes and spatial relationships of geographic objects for multivariate spatial clustering. A subregion is created for each geographic object serving as the basic unit when performing clustering. A Markov random field is then constructed to characterize the attribute dependencies of subregions. Using a spatial consistency strategy, nearby objects are encouraged to belong to the same cluster. To test the performance of the proposed STICC algorithm, we apply it in two use cases. The comparison results with several baseline methods show that the STICC outperforms others significantly in terms of adjusted rand index and macro-F1 score. Join count statistics is also calculated and shows that the spatial contiguity is well preserved by STICC. Such a spatial clustering method may benefit various applications in the fields of geography, remote sensing, transportation, and urban planning, etc.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">kang2022sticc</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{STICC: a multivariate spatial clustering method for repeated geographic pattern discovery with consideration of spatial contiguity}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kang, Yuhao and Wu, Kunlin and Gao, Song and Ng, Ignavier and Rao, Jinmeng and Ye, Shan and Zhang, Fan and Fei, Teng}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Journal of Geographical Information Science}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{36}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{8}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1518--1549}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Taylor \&amp; Francis}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/stcl-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/stcl-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/stcl-1400.webp"></source> <img src="/assets/img/publication_preview/stcl.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="stcl.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="peng2021spatiotemporal" class="col-sm-7"> <div class="abbr"><abbr class="badge">IEEE IGARSS</abbr></div> <div class="title">Spatiotemporal contrastive representation learning for building damage classification</div> <div class="author"> Bo Peng, Qunying Huang, and <em>Jinmeng Rao</em> </div> <div class="periodical"> <em>In 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS</em>, 2021 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b>Highlight: Spatiotemporal Contrastive Representation Learning; Acceptance rate 34.2%</b> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/9554302" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Automatic building damage assessment after natural disasters is important for emergency response. While existing supervised deep learning models achieved good performance on building damage classification, these models require massive human labels for training. Additionally, pre-trained models often fail to generalize well to new disaster events due to gaps between domains associated with training and testing data. In response, this study proposes a novel spatiotemporal contrastive representation learning model for learning features of building damages with big unlabeled data. Experimental results demonstrate superior performance of such features on classifying building damages resulting from various natural disasters (e.g., hurricanes, floods, wildfires, earthquakes, etc.) across different geographic locations worldwide, compared with the state-of-the-art supervised methods.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">peng2021spatiotemporal</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Spatiotemporal contrastive representation learning for building damage classification}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Peng, Bo and Huang, Qunying and Rao, Jinmeng}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{8562--8565}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Highlight: Spatiotemporal Contrastive Representation Learning; Acceptance rate 34.2\%}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/vtsv.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/vtsv.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/vtsv.gif-1400.webp"></source> <img src="/assets/img/publication_preview/vtsv.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="vtsv.gif" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="rao2021vtsv" class="col-sm-7"> <div class="abbr"><abbr class="badge">ACM SIGSPATIAL GEOAI</abbr></div> <div class="title">VTSV: A privacy-preserving vehicle trajectory simulation and visualization platform using deep reinforcement learning</div> <div class="author"> <em>Jinmeng Rao</em>, Song Gao, and Xiaojin Zhu</div> <div class="periodical"> <em>In Proceedings of the 4th ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery</em>, 2021 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b>Highlight: Deep Reinforcement Learning; Personalized Driving Simulation <a href="https://www.youtube.com/watch?v=NY5L4bu2kTU" rel="external nofollow noopener" target="_blank">[ Video Demo ]</a></b> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://dl.acm.org/doi/10.1145/3486635.3491073" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Trajectory data is among the most sensitive data and the society increasingly raises privacy concerns. In this demo paper, we present a privacy-preserving Vehicle Trajectory Simulation and Visualization (VTSV) web platform (demo video: https://youtu.be/NY5L4bu2kTU), which automatically generates navigation routes between given pairs of origins and destinations and employs a deep reinforcement learning model to simulate vehicle trajectories with customized driving behaviors such as normal driving, overspeed, aggressive acceleration, and aggressive turning. The simulated vehicle trajectory data contain high-sample-rate of attributes including GPS location, speed, acceleration, and steering angle, and such data are visualized in VTSV using streetscape.gl, an autonomous driving data visualization framework. Location privacy protection methods such as origin-destination geomasking and trajectory k-anonymity are integrated into the platform to support privacy-preserving trajectory data generation and publication. We design two application scenarios to demonstrate how VTSV performs location privacy protection and customize driving behavior, respectively. The demonstration shows that VTSV is able to mitigate data privacy, sparsity, and imbalance sampling issues, which offers new insights into driving trajectory simulation and GeoAI-powered privacy-preserving data publication.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">rao2021vtsv</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{VTSV: A privacy-preserving vehicle trajectory simulation and visualization platform using deep reinforcement learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rao, Jinmeng and Gao, Song and Zhu, Xiaojin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 4th ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{43--46}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Highlight: Deep Reinforcement Learning; Personalized Driving Simulation &lt;a href="https://www.youtube.com/watch?v=NY5L4bu2kTU"&gt;[ Video Demo ]&lt;/a&gt;}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/fllocrec-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/fllocrec-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/fllocrec-1400.webp"></source> <img src="/assets/img/publication_preview/fllocrec.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="fllocrec.jpg" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="rao2021privacy" class="col-sm-7"> <div class="abbr"><abbr class="badge">TGIS</abbr></div> <div class="title">A privacy-preserving framework for location recommendation using decentralized collaborative machine learning</div> <div class="author"> <em>Jinmeng Rao</em>, Song Gao, Mingxiao Li, and Qunying Huang</div> <div class="periodical"> <em>Transactions in GIS</em>, 2021 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b>Highlight: Flagship Journal in Geographic Information Systems; Recognized as the first work on Federated Learning + Spatial Data Sciences <a href="https://publications.ait.ac.at/en/publications/on-the-role-of-spatial-data-science-for-federated-learning" rel="external nofollow noopener" target="_blank">(Graser, A. et al 2022)</a></b> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/tgis.12769" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/GeoDS/FL-LocRec" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>The nowadays ubiquitous location-aware mobile devices have contributed to the rapid growth of individual-level location data. Such data are usually collected by location-based service platforms as training data to improve their predictive models’ performance, but the collection of such data may raise public concerns about privacy issues. In this study, we introduce a privacy-preserving location recommendation framework based on a decentralized collaborative machine learning approach: federated learning. Compared with traditional centralized learning frameworks, we keep users’ data on their own devices and train the model locally so that their data remain private. The local model parameters are aggregated and updated through secure multiple-party computation to achieve collaborative learning among users while preserving privacy. Our framework also integrates information about transportation infrastructure, place safety, and flow-based spatial interaction to further improve recommendation accuracy. We further design two attack cases to examine the privacy protection effectiveness and robustness of the framework. The results show that our framework achieves a better balance on the privacy–utility trade-off compared with traditional centralized learning methods. The results and ensuing discussion offer new insights into privacy-preserving geospatial artificial intelligence and promote geoprivacy in location-based services.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">rao2021privacy</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A privacy-preserving framework for location recommendation using decentralized collaborative machine learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rao, Jinmeng and Gao, Song and Li, Mingxiao and Huang, Qunying}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Transactions in GIS}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Highlight: Flagship Journal in Geographic Information Systems; Recognized as the first work on Federated Learning + Spatial Data Sciences &lt;a href="https://publications.ait.ac.at/en/publications/on-the-role-of-spatial-data-science-for-federated-learning"&gt;(Graser, A. et al 2022)&lt;/a&gt;}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/book_road_ext-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/book_road_ext-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/book_road_ext-1400.webp"></source> <img src="/assets/img/publication_preview/book_road_ext.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="book_road_ext.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="gao2021automatic" class="col-sm-7"> <div class="abbr"><abbr class="badge">BOOK CHAPTER</abbr></div> <div class="title">Automatic urban road network extraction from massive GPS trajectories of taxis</div> <div class="author"> Song Gao, Mingxiao Li, <em>Jinmeng Rao</em>, Gengchen Mai, Timothy Prestby, Joseph Marks, and Yingjie Hu</div> <div class="periodical"> <em>In Handbook of Big Geospatial Data</em>, 2021 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b></b> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/chapter/10.1007/978-3-030-55462-0_11" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Urban road networks are fundamental transportation infrastructures in daily life and essential in digital maps to support vehicle routing and navigation. Traditional methods of map vector data generation based on surveyor’s field work and map digitalization are costly and have a long update period. In the Big Data age, large-scale GPS-enabled taxi trajectories and high-volume ride-sharing datasets are increasingly available. These datasets provide high-resolution spatiotemporal information about urban traffic along road networks. In this study, we present a novel geospatial-big-data-driven framework that includes trajectory compression, clustering, and vectorization to automatically generate urban road geometric information. A case study is conducted using a large-scale DiDi ride-sharing GPS dataset in the city of Chengdu in China. We compare the results of our automatic extraction method with the road layer downloaded from OpenStreetMap. We measure the quality and demonstrate the effectiveness of our road extraction method regarding accuracy, spatial coverage and connectivity. The proposed framework shows a good potential to update fundamental road transportation information for smart-city development and intelligent transportation management using geospatial big data.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@incollection</span><span class="p">{</span><span class="nl">gao2021automatic</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Automatic urban road network extraction from massive GPS trajectories of taxis}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao, Song and Li, Mingxiao and Rao, Jinmeng and Mai, Gengchen and Prestby, Timothy and Marks, Joseph and Hu, Yingjie}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Handbook of Big Geospatial Data}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{261--283}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer International Publishing Cham}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/lup_housing-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/lup_housing-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/lup_housing-1400.webp"></source> <img src="/assets/img/publication_preview/lup_housing.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="lup_housing.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="kang2021understanding" class="col-sm-7"> <div class="abbr"><abbr class="badge">LUP</abbr></div> <div class="title">Understanding house price appreciation using multi-source big geo-data and machine learning</div> <div class="author"> Yuhao Kang, Fan Zhang, Wenzhe Peng, Song Gao, <em>Jinmeng Rao</em>, Fabio Duarte, and Carlo Ratti</div> <div class="periodical"> <em>Land Use Policy</em>, 2021 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b></b> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0264837719316746" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Understanding house price appreciation benefits place-based decision makings and real estate market analyses. Although large amounts of interests have been paid in the house price modeling, limited work has focused on evaluating the price appreciation rate. In this study, we propose a data-fusion framework to examine how well house price appreciation potentials can be predicted by combining multiple data sources. We used data sets including house structural attributes, house photos, locational amenities, street view images, transportation accessibility, visitor patterns, and socioeconomic attributes of neighborhoods to enrich our understanding of the real estate appreciation and its predictive modeling. As a case study, we investigate more than 20,000 houses in the Greater Boston Area, and discuss the spatial dependency of house price appreciations, influential variables and their relationships. In detail, we extract deep features from street view images and house photos using a deep learning model, merging features from multi-source data and modeling house price appreciation using machine learning models and the geographically weighted regression at two spatial scales: fine-scale point level and aggregated neighborhood level. Results show that the house price appreciation rate can be modeled with high accuracy using the proposed framework (R2 = 0.74 for gradient boosting machine at neighborhood-scale). We discovered that houses with low house prices and small house areas may have a higher house appreciation potential. Our results provide insights into how multi-source big geo-data can be employed in machine learning frameworks to characterize real estate price trends and help understand human settlements for policy-making.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">kang2021understanding</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Understanding house price appreciation using multi-source big geo-data and machine learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kang, Yuhao and Zhang, Fan and Peng, Wenzhe and Gao, Song and Rao, Jinmeng and Duarte, Fabio and Ratti, Carlo}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Land Use Policy}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{111}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{104919}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Pergamon}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/pnas-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/pnas-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/pnas-1400.webp"></source> <img src="/assets/img/publication_preview/pnas.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="pnas.jpg" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="hou2021intracounty" class="col-sm-7"> <div class="abbr"><abbr class="badge">PNAS</abbr></div> <div class="title">Intracounty modeling of COVID-19 infection with human mobility: Assessing spatial heterogeneity with business traffic, age, and race</div> <div class="author"> Xiao Hou, Song Gao, Qin Li, Yuhao Kang, Nan Chen, Kaiping Chen, <em>Jinmeng Rao</em>, Jordan S Ellenberg, and Jonathan A Patz</div> <div class="periodical"> <em>Proceedings of the National Academy of Sciences</em>, 2021 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b></b> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.pnas.org/doi/10.1073/pnas.2020524118" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/GeoDS/IntraCounty-Mobility-SEIR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>The COVID-19 pandemic is a global threat presenting health, economic, and social challenges that continue to escalate. Metapopulation epidemic modeling studies in the susceptible–exposed–infectious–removed (SEIR) style have played important roles in informing public health policy making to mitigate the spread of COVID-19. These models typically rely on a key assumption on the homogeneity of the population. This assumption certainly cannot be expected to hold true in real situations; various geographic, socioeconomic, and cultural environments affect the behaviors that drive the spread of COVID-19 in different communities. What’s more, variation of intracounty environments creates spatial heterogeneity of transmission in different regions. To address this issue, we develop a human mobility flow-augmented stochastic SEIR-style epidemic modeling framework with the ability to distinguish different regions and their corresponding behaviors. This modeling framework is then combined with data assimilation and machine learning techniques to reconstruct the historical growth trajectories of COVID-19 confirmed cases in two counties in Wisconsin. The associations between the spread of COVID-19 and business foot traffic, race and ethnicity, and age structure are then investigated. The results reveal that, in a college town (Dane County), the most important heterogeneity is age structure, while, in a large city area (Milwaukee County), racial and ethnic heterogeneity becomes more apparent. Scenario studies further indicate a strong response of the spread rate to various reopening policies, which suggests that policy makers may need to take these heterogeneities into account very carefully when designing policies for mitigating the ongoing spread of COVID-19 and reopening.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hou2021intracounty</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Intracounty modeling of COVID-19 infection with human mobility: Assessing spatial heterogeneity with business traffic, age, and race}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hou, Xiao and Gao, Song and Li, Qin and Kang, Yuhao and Chen, Nan and Chen, Kaiping and Rao, Jinmeng and Ellenberg, Jordan S and Patz, Jonathan A}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proceedings of the National Academy of Sciences}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{118}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{24}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{e2020524118}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{National Academy of Sciences}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/planning-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/planning-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/planning-1400.webp"></source> <img src="/assets/img/publication_preview/planning.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="planning.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="peng2021urban" class="col-sm-7"> <div class="abbr"><abbr class="badge">IEEE T-ITS</abbr></div> <div class="title">Urban multiple route planning model using dynamic programming in reinforcement learning</div> <div class="author"> Ningyezi Peng, Yuliang Xi, <em>Jinmeng Rao</em>, Xiangyuan Ma, and Fu Ren</div> <div class="periodical"> <em>IEEE Transactions on Intelligent Transportation Systems</em>, 2021 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b></b> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/9440854" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>With the development of the economy and the acceleration of urbanization, traffic congestion has become a worldwide problem. Advances in mobile Internet and sensor technologies have increased real-time data sharing, providing a new opportunity for urban route planning. However, due to the difficulty of handling complex global information, making correct decisions in large-scale and complex traffic environments is a problem that urgently needs to be solved. In this paper, a multiple route planning model (multi-route dynamic programming (DP) model) is proposed to solve the urban route planning problem with traffic flow information. In particular, we adopt the DP algorithm in this model, design a reward function suitable for urban path planning problems, and generate multiple routes based on the Q values. In addition, we design different scenarios using real-world road networks to test our model. Through the experiments, we demonstrate that our model has the potential to yield optimal results under large-scale scenarios with high efficiency. The advantages of integrating the distance contribution index (DCI) in the reward function are also elaborated. Moreover, our model can provide alternative routes to divert traffic from the optimal route, thus mitigating the congestion drift problem.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">peng2021urban</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Urban multiple route planning model using dynamic programming in reinforcement learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Peng, Ningyezi and Xi, Yuliang and Rao, Jinmeng and Ma, Xiangyuan and Ren, Fu}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Intelligent Transportation Systems}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{23}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{8037--8047}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/jgsa.webp-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/jgsa.webp-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/jgsa.webp-1400.webp"></source> <img src="/assets/img/publication_preview/jgsa.webp" class="preview z-depth-1 rounded" width="auto" height="auto" alt="jgsa.webp" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="rao2021multi" class="col-sm-7"> <div class="abbr"><abbr class="badge">JGSA</abbr></div> <div class="title">A multi-perspective narrative-based geovisualization dashboard for the 2020 US presidential election</div> <div class="author"> <em>Jinmeng Rao</em>, Kexin Chen, Ellie Fan Yang, Jacob Kruse, Kyler Hudson, and Song Gao</div> <div class="periodical"> <em>Journal of Geovisualization and Spatial Analysis</em>, 2021 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b></b> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/article/10.1007/s41651-021-00087-6" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://www.arcgis.com/apps/dashboards/eb6a099bc74a45ba82971f731d9b717d" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>In this paper, we design and implement a map dashboard that combines spatio-temporal visualization and interactive narrative to comprehensively illustrate the 2020 US presidential election. Specifically, our dashboard takes campaign rallies and major events as narrative clues and integrates multi-perspective factors (e.g., the spatial spread of COVID-19, social distancing adherence, poll results) for visualization and statistical analysis. Compared with traditional methods and products, our integrated multi-perspective solution better balances the narrative property and the geovisualization property of a dashboard, making it suitable for illustrating social or political events that happened on a large geographic scale. The result shows that our narrative-based geovisualization dashboard may be used for demonstrating and associating multiple factors with partisanship and has the potential to help users explore the interaction between policies controlling COVID-19, social distancing, and partisanship across the country during the 2020 US presidential election.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">rao2021multi</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A multi-perspective narrative-based geovisualization dashboard for the 2020 US presidential election}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rao, Jinmeng and Chen, Kexin and Yang, Ellie Fan and Kruse, Jacob and Hudson, Kyler and Gao, Song}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Geovisualization and Spatial Analysis}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--15}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer International Publishing}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/trajgan-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/trajgan-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/trajgan-1400.webp"></source> <img src="/assets/img/publication_preview/trajgan.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="trajgan.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="rao2020lstm" class="col-sm-7"> <div class="abbr"><abbr class="badge">GISCIENCE</abbr></div> <div class="title">LSTM-TrajGAN: A deep learning approach to trajectory privacy protection</div> <div class="author"> <em>Jinmeng Rao</em>, Song Gao, Yuhao Kang, and Qunying Huang</div> <div class="periodical"> <em>In 11th International Conference on Geographic Information Science (GIScience 2021) - Part I</em>, 2020 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b>Highlight: Top Conference in Geographic Information Sciences; Acceptance rate 34%</b> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://drops.dagstuhl.de/opus/volltexte/2020/13047/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/GeoDS/LSTM-TrajGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>The prevalence of location-based services contributes to the explosive growth of individual-level location trajectory data and raises public concerns about privacy issues. In this research, we propose a novel LSTM-TrajGAN approach, which is an end-to-end deep learning model to generate privacy-preserving synthetic trajectory data for data sharing and publication. We design a loss metric function TrajLoss to measure the trajectory similarity losses for model training and optimization. The model is evaluated on the trajectory-user-linking task on a real-world semantic trajectory dataset. Compared with other common geomasking methods, our model can better prevent users from being re-identified, and it also preserves essential spatial, temporal, and thematic characteristics of the real trajectory data. The model better balances the effectiveness of trajectory privacy protection and the utility for spatial and temporal analyses, which offers new insights into the GeoAI-powered privacy protection for human mobility studies.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">rao2020lstm</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{LSTM-TrajGAN: A deep learning approach to trajectory privacy protection}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rao, Jinmeng and Gao, Song and Kang, Yuhao and Huang, Qunying}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{11th International Conference on Geographic Information Science (GIScience 2021) - Part I}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{177}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{12--1}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{Schloss Dagstuhl--Leibniz-Zentrum f$\{$$\backslash$"u$\}$r Informatik}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Highlight: Top Conference in Geographic Information Sciences; Acceptance rate 34\%}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/mobmap.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/mobmap.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/mobmap.gif-1400.webp"></source> <img src="/assets/img/publication_preview/mobmap.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="mobmap.gif" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="gao2020mapping" class="col-sm-7"> <div class="abbr"><abbr class="badge">ACM SIGSPATIAL</abbr></div> <div class="title">Mapping county-level mobility pattern changes in the United States in response to COVID-19</div> <div class="author"> Song Gao, <em>Jinmeng Rao</em>, Yuhao Kang, Yunlei Liang, and Jake Kruse</div> <div class="periodical"> <em>SIGSpatial Special</em>, 2020 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b>Highlight: The first work on social distancing mapping during COVID-19; Media Coverage <a href="https://www.nbc15.com/2020/10/07/tracking-movement-in-wisconsin-leading-up-to-latest-health-order/" rel="external nofollow noopener" target="_blank">NBC</a>, <a href="http://fox47.com/news/local/uw-madison-researchers-map-travel-data-to-combat-pandemic" rel="external nofollow noopener" target="_blank">FOX</a>, <a href="https://www.wisn.com/article/uw-madison-scientists-develop-social-distancing-tracker-map/32163419#" rel="external nofollow noopener" target="_blank">ABC</a></b> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://dl.acm.org/doi/10.1145/3404820.3404824" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/GeoDS/mapmobility" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://geography.wisc.edu/covid19/physical-distancing/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>To contain the COVID-19 epidemic, one of the non-pharmacological epidemic control measures is reducing the transmission rate of SARS-COV-2 in the population through social distancing. An interactive web-based mapping platform that provides timely quantitative information on how people in different counties and states reacted to the social distancing guidelines was developed by the GeoDS Lab @UW-Madison with the support of the National Science Foundation RAPID program. The web portal integrates geographic information systems (GIS) and daily updated human mobility statistical patterns (median travel distance and stay-at-home dwell time) derived from large-scale anonymized and aggregated smartphone location big data at the county-level in the United States, and aims to increase risk awareness of the public, support data-driven public health and governmental decision-making, and help enhance community responses to the COVID-19 pandemic.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">gao2020mapping</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Mapping county-level mobility pattern changes in the United States in response to COVID-19}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao, Song and Rao, Jinmeng and Kang, Yuhao and Liang, Yunlei and Kruse, Jake}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{SIGSpatial Special}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{12}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{16--26}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{ACM New York, NY, USA}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Highlight: The first work on social distancing mapping during COVID-19; Media Coverage &lt;a href="https://www.nbc15.com/2020/10/07/tracking-movement-in-wisconsin-leading-up-to-latest-health-order/"&gt;NBC&lt;/a&gt;, &lt;a href="http://fox47.com/news/local/uw-madison-researchers-map-travel-data-to-combat-pandemic"&gt;FOX&lt;/a&gt;, &lt;a href="https://www.wisn.com/article/uw-madison-scientists-develop-social-distancing-tracker-map/32163419#"&gt;ABC&lt;/a&gt;}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/autocarto_encoding-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/autocarto_encoding-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/autocarto_encoding-1400.webp"></source> <img src="/assets/img/publication_preview/autocarto_encoding.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="autocarto_encoding.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="kang2020towards" class="col-sm-7"> <div class="abbr"><abbr class="badge">AUTOCARTO</abbr></div> <div class="title">Towards cartographic knowledge encoding with deep learning: A case study of building generalization</div> <div class="author"> Y Kang, J Rao, W Wang, B Peng, S Gao, and F Zhang</div> <div class="periodical"> <em>In Proceedings of the AutoCarto</em>, 2020 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b></b> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://cartogis.org/docs/autocarto/2020/docs/abstracts/2b%20Towards%20Cartographic%20Knowledge%20Encoding%20with%20Deep%20Learning%20A.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">kang2020towards</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Towards cartographic knowledge encoding with deep learning: A case study of building generalization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kang, Y and Rao, J and Wang, W and Peng, B and Gao, S and Zhang, F}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the AutoCarto}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/autocarto_ar-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/autocarto_ar-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/autocarto_ar-1400.webp"></source> <img src="/assets/img/publication_preview/autocarto_ar.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="autocarto_ar.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="rao2020landmarks" class="col-sm-7"> <div class="abbr"><abbr class="badge">AUTOCARTO</abbr></div> <div class="title">Landmarks as Beacons: Pedestrian Navigation Based on Landmark Detection and Mobile Augmented Reality</div> <div class="author"> J Rao, S Gao, Y Kang, and Q Du</div> <div class="periodical"> <em>In Proceedings of the AutoCarto</em>, 2020 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b></b> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://cartogis.org/docs/autocarto/2020/docs/abstracts/4i%20Landmarks%20as%20Beacons%20Pedestrian%20Navigation%20Based%20on%20Landmark%20Detection%20and%20Mobile%20Augmented%20Reality.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">rao2020landmarks</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Landmarks as Beacons: Pedestrian Navigation Based on Landmark Detection and Mobile Augmented Reality}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rao, J and Gao, S and Kang, Y and Du, Q}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the AutoCarto}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/sd_flows-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/sd_flows-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/sd_flows-1400.webp"></source> <img src="/assets/img/publication_preview/sd_flows.jpeg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="sd_flows.jpeg" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="kang2020multiscale" class="col-sm-7"> <div class="abbr"><abbr class="badge">SCIENTIFIC DATA</abbr></div> <div class="title">Multiscale dynamic human mobility flow dataset in the US during the COVID-19 epidemic</div> <div class="author"> Yuhao Kang, Song Gao, Yunlei Liang, Mingxiao Li, <em>Jinmeng Rao</em>, and Jake Kruse</div> <div class="periodical"> <em>Scientific data</em>, 2020 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b></b> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.nature.com/articles/s41597-020-00734-5" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/GeoDS/COVID19USFlows" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Understanding dynamic human mobility changes and spatial interaction patterns at different geographic scales is crucial for assessing the impacts of non-pharmaceutical interventions (such as stay-at-home orders) during the COVID-19 pandemic. In this data descriptor, we introduce a regularly-updated multiscale dynamic human mobility flow dataset across the United States, with data starting from March 1st, 2020. By analysing millions of anonymous mobile phone users’ visits to various places provided by SafeGraph, the daily and weekly dynamic origin-to-destination (O-D) population flows are computed, aggregated, and inferred at three geographic scales: census tract, county, and state. There is high correlation between our mobility flow dataset and openly available data sources, which shows the reliability of the produced data. Such a high spatiotemporal resolution human mobility flow dataset at different geographic scales over time may help monitor epidemic spreading dynamics, inform public health policy, and deepen our understanding of human behaviour changes under the unprecedented public health crisis. This up-to-date O-D flow open data can support many other social sensing and transportation applications.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">kang2020multiscale</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multiscale dynamic human mobility flow dataset in the US during the COVID-19 epidemic}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kang, Yuhao and Gao, Song and Liang, Yunlei and Li, Mingxiao and Rao, Jinmeng and Kruse, Jake}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Scientific data}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{390}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Nature Publishing Group UK London}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/jama-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/jama-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/jama-1400.webp"></source> <img src="/assets/img/publication_preview/jama.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="jama.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="gao2020association" class="col-sm-7"> <div class="abbr"><abbr class="badge">JAMA</abbr></div> <div class="title">Association of mobile phone location data indications of travel and stay-at-home mandates with covid-19 infection rates in the us</div> <div class="author"> Song Gao, <em>Jinmeng Rao</em>, Yuhao Kang, Yunlei Liang, Jake Kruse, Dorte Dopfer, Ajay K Sethi, Juan Francisco Mandujano Reyes, Brian S Yandell, and Jonathan A Patz</div> <div class="periodical"> <em>JAMA network open</em>, 2020 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b></b> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2770249" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Importance A stay-at-home social distancing mandate is a key nonpharmacological measure to reduce the transmission rate of severe acute respiratory syndrome coronavirus-2 (SARS-CoV-2), but a high rate of adherence is needed. Objective To examine the association between the rate of human mobility changes and the rate of confirmed cases of SARS-CoV-2 infection. Design, Setting, and Participants This cross-sectional study used daily travel distance and home dwell time derived from millions of anonymous mobile phone location data from March 11 to April 10, 2020, provided by the Descartes Labs and SafeGraph to quantify the degree to which social distancing mandates were followed in the 50 US states and District of Columbia and the association of mobility changes with rates of coronavirus disease 2019 (COVID-19) cases. Exposure State-level stay-at-home orders during the COVID-19 pandemic. Main Outcomes and Measures The main outcome was the association of state-specific rates of COVID-19 confirmed cases with the change rates of median travel distance and median home dwell time of anonymous mobile phone users. The increase rates are measured by the exponent in curve fitting of the COVID-19 cumulative confirmed cases, while the mobility change (increase or decrease) rates were measured by the slope coefficient in curve fitting of median travel distance and median home dwell time for each state. Results Data from more than 45 million anonymous mobile phone devices were analyzed. The correlation between the COVID-19 increase rate and travel distance decrease rate was –0.586 (95% CI, –0.742 to –0.370) and the correlation between COVID-19 increase rate and home dwell time increase rate was 0.526 (95% CI, 0.293 to 0.700). Increases in state-specific doubling time of total cases ranged from 1.0 to 6.9 days (median [interquartile range], 2.7 [2.3-3.3] days) before stay-at-home orders were enacted to 3.7 to 30.3 days (median [interquartile range], 6.0 [4.8-7.1] days) after stay-at-home social distancing orders were put in place, consistent with pandemic modeling results. Conclusions and Relevance These findings suggest that stay-at-home social distancing mandates, when they were followed by measurable mobility changes, were associated with reduction in COVID-19 spread. These results come at a particularly critical period when US states are beginning to relax social distancing policies and reopen their economies. These findings support the efficacy of social distancing and could help inform future implementation of social distancing policies should they need to be reinstated during later periods of COVID-19 reemergence.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">gao2020association</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Association of mobile phone location data indications of travel and stay-at-home mandates with covid-19 infection rates in the us}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao, Song and Rao, Jinmeng and Kang, Yuhao and Liang, Yunlei and Kruse, Jake and Dopfer, Dorte and Sethi, Ajay K and Reyes, Juan Francisco Mandujano and Yandell, Brian S and Patz, Jonathan A}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{JAMA network open}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{9}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{e2020485--e2020485}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{American Medical Association}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/ceus-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/ceus-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/ceus-1400.webp"></source> <img src="/assets/img/publication_preview/ceus.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="ceus.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="wu2020assessing" class="col-sm-7"> <div class="abbr"><abbr class="badge">CEUS</abbr></div> <div class="title">Assessing multiscale visual appearance characteristics of neighbourhoods using geographically weighted principal component analysis in Shenzhen, China</div> <div class="author"> Chao Wu, Ningyezi Peng, Xiangyuan Ma, Sheng Li, and <em>Jinmeng Rao*</em> </div> <div class="periodical"> <em>Computers, Environment and Urban Systems</em>, 2020 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b></b> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0198971520302805" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>The neighbourhood is a basic residential unit and is characterized by its physical setting, functional attributes and visual appearance. The visual appearance of a neighbourhood can directly affect the impression of humans regarding the local living environment. Assessing the characteristics of the visual appearance of a neighbourhood is significance for promoting people’s physical activities, improving residents’ sense of comfort and even ensuring the equality of facilities. However, studies assessing the spatial characteristics of visual appearance are still limited. Therefore, this article applies street view images to quantify the visual appearance of neighbourhoods at multiple scales in Shenzhen, China. Then, geographically weighted principal component analysis (GWPCA) is employed to explore the varying multivariate structures of visual appearance. The results confirm that GWPCA can be effective in assessing the visual appearance characteristics of neighbourhoods while considering spatial heterogeneity. The visual appearance characteristics of neighbourhoods are sensitive to both the spatial location and analysis scale. The extracted geographically weighted principal components (GWPCs) can represent the original screen elements by emphasizing certain comprehensive concepts, such as walkability, accessibility and vibrancy. The exploratory findings of this article allow for an improvement of studies on spatial quality at the human scale and could potentially guide neighbourhood planning and street design.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">wu2020assessing</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Assessing multiscale visual appearance characteristics of neighbourhoods using geographically weighted principal component analysis in Shenzhen, China}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wu, Chao and Peng, Ningyezi and Ma, Xiangyuan and Li, Sheng and Rao*, Jinmeng}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computers, Environment and Urban Systems}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{84}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{101547}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Pergamon}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/python_geo-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/python_geo-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/python_geo-1400.webp"></source> <img src="/assets/img/publication_preview/python_geo.jpeg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="python_geo.jpeg" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="du2020geo" class="col-sm-7"> <div class="abbr"><abbr class="badge">CHINESE</abbr></div> <div class="title">Multi-source Geographic Data Efficient Quality Inspection System Based on Python</div> <div class="author"> Tian Du, Dalu Xu, Xiaojuan Zhu, <em>Jinmeng Rao</em>, and Qingyun Du</div> <div class="periodical"> <em></em> 2020 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b></b> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://www.xml-data.org/CHDLXX/html/f2e23850-c15d-4e7b-8aba-5ac6831cc28c.htm" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/JinmengRao/Geospatial-Data-Processing-System" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>For the ever-increasing massive geospatial big data and increasingly complex data quality inspection requirements, we design a comprehensive spatial data quality inspection system based on Python and QT framework. We use open source third-party tool libraries and independent research algorithms to process multiple types of spatial geographic big data, and combine with multi-process and multi-threading technology to improve quality inspection efficiency. The system uses the infrastructure development to achieve the indicator items of data quality inspection. The system can import flexible quality inspection rules, and export quality inspection logs and quality inspection results reports, and verify the feasibility, expandability and efficiency of the system in production practice.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">du2020geo</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multi-source Geographic Data Efficient Quality Inspection System Based on Python}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Du, Tian and Xu, Dalu and Zhu, Xiaojuan and Rao, Jinmeng and Du, Qingyun}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Journal of Geomatics}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/raas-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/raas-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/raas-1400.webp"></source> <img src="/assets/img/publication_preview/raas.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="raas.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="liang2019analyzing" class="col-sm-7"> <div class="abbr"><abbr class="badge">ACM SIGSPATIAL RAAS</abbr></div> <div class="title">Analyzing the Gap Between Ride-hailing Location and Pick-up Location with Geographical Contexts</div> <div class="author"> Yunlei Liang, Song Gao, Mingxiao Li, Yuhao Kang, and <em>Jinmeng Rao</em> </div> <div class="periodical"> <em>In Proceedings of the 1st ACM SIGSPATIAL International Workshop on Ride-hailing Algorithms, Applications, and Systems</em>, 2019 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b></b> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://geography.wisc.edu/geods/wp-content/uploads/sites/28/2022/05/2019_RAAS_DidiLocationGap.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Location uncertainty plays a key role in location-based services and applications. This research mainly focuses on the problem of location uncertainty when users are using the ride-hailing applications from the perspective of geographical contexts. The distance gap between the ride-hailing identified search location and the actual pick-up location was calculated and used as the measurement of location uncertainty. It may come from GPS noise or the gap between the current location of a passenger (eg, inside a building or a university campus) and the actual pick-up location along the streets for a ride. For the geographical contexts, this study considers factors including population density, road density, various kinds of Points Of Interests (POIs), and also the frequency of ride-hailing requests given a region. By using regression analysis techniques to find the relation between the geographical contexts and the distance gap, this study identifies potential factors (eg, enterprise buildings, dense roads, and highly populated areas) that affect the location uncertainty the most.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">liang2019analyzing</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Analyzing the Gap Between Ride-hailing Location and Pick-up Location with Geographical Contexts}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liang, Yunlei and Gao, Song and Li, Mingxiao and Kang, Yuhao and Rao, Jinmeng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 1st ACM SIGSPATIAL International Workshop on Ride-hailing Algorithms, Applications, and Systems}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/josis-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/josis-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/josis-1400.webp"></source> <img src="/assets/img/publication_preview/josis.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="josis.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="gao2019exploring" class="col-sm-7"> <div class="abbr"><abbr class="badge">JOSIS</abbr></div> <div class="title">Exploring the effectiveness of geomasking techniques for protecting the geoprivacy of Twitter users</div> <div class="author"> Song Gao, <em>Jinmeng Rao</em>, Xinyi Liu, Yuhao Kang, Qunying Huang, and Joseph App</div> <div class="periodical"> <em>Journal of Spatial Information Science</em>, 2019 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b></b> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://josis.org/index.php/josis/article/view/107" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>With the ubiquitous use of location-based services, large-scale individual-level location data has been widely collected through location-awareness devices. Geoprivacy concerns arise on the issues of user identity de-anonymization and location exposure. In this work, we investigate the effectiveness of geomasking techniques for protecting the geoprivacy of active Twitter users who frequently share geotagged tweets in their home and work locations. By analyzing over 38,000 geotagged tweets of 93 active Twitter users in three U.S. cities, the two-dimensional Gaussian masking technique with proper standard deviation settings is found to be more effective to protect user’s location privacy while sacrificing geospatial analytical resolution than the random perturbation masking method and the aggregation on traffic analysis zones. Furthermore, a three-dimensional theoretical framework considering privacy, analytics, and uncertainty factors simultaneously is proposed to assess geomasking techniques. Our research offers insights into geoprivacy concerns of social media users’ georeferenced data sharing for future development of location-based applications and services.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">gao2019exploring</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Exploring the effectiveness of geomasking techniques for protecting the geoprivacy of Twitter users}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao, Song and Rao, Jinmeng and Liu, Xinyi and Kang, Yuhao and Huang, Qunying and App, Joseph}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Spatial Information Science}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{19}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{105--129}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/cn_oar-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/cn_oar-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/cn_oar-1400.webp"></source> <img src="/assets/img/publication_preview/cn_oar.jpeg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="cn_oar.jpeg" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="qiao2017outdoorar" class="col-sm-7"> <div class="abbr"><abbr class="badge">CHINESE</abbr></div> <div class="title">Geographic Object Detection for Outdoor Augmented Reality</div> <div class="author"> Yanjun Qiao, <em>Jinmeng Rao</em>, Junxing Wang, Qingyun Du, and Fu Ren</div> <div class="periodical"> <em>Geomatics World</em>, 2017 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b></b> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://en.cnki.com.cn/KCMS/detail/detail.aspx?dbcode=CJFD&amp;dbname=CJFDLAST2017&amp;filename=CHRK201705011&amp;uniplatform=OVERSEA&amp;v=rwzjvYnfVdJ5IFglfb_ohKRBYi-wdsbPFflrATZvXrcyr6rQyZACZ8QmrG_Xp15S" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>This paper studies the convolutional neural network model for image classification and object detection in the field of computer vision, and modifies an object detection model which combines Res Net structures to solve the problem of geographic object detection in outdoor augmented reality. An outdoor augmented reality system is developed with the use of the proposed model trained as an object detection engine, which achieves the near-real-time performance and high accuracy.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">qiao2017outdoorar</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Geographic Object Detection for Outdoor Augmented Reality}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Qiao, Yanjun and Rao, Jinmeng and Wang, Junxing and Du, Qingyun and Ren, Fu}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Geomatics World}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{24}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{51--55}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"><div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/sensors_moar-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/sensors_moar-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/sensors_moar-1400.webp"></source> <img src="/assets/img/publication_preview/sensors_moar.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="sensors_moar.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></div> <div id="rao2017mobile" class="col-sm-7"> <div class="abbr"><abbr class="badge">SENSORS</abbr></div> <div class="title">A mobile outdoor augmented reality method combining deep learning object detection and spatial relationships for geovisualization</div> <div class="author"> <em>Jinmeng Rao</em>, Yanjun Qiao, Fu Ren, Junxing Wang, and Qingyun Du</div> <div class="periodical"> <em>Sensors</em>, 2017 </div> <div class="periodical" style="color:rgb(255, 0, 0);"> <b></b> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.mdpi.com/1424-8220/17/9/1951" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>The purpose of this study was to develop a robust, fast and markerless mobile augmented reality method for registration, geovisualization and interaction in uncontrolled outdoor environments. We propose a lightweight deep-learning-based object detection approach for mobile or embedded devices; the vision-based detection results of this approach are combined with spatial relationships by means of the host device’s built-in Global Positioning System receiver, Inertial Measurement Unit and magnetometer. Virtual objects generated based on geospatial information are precisely registered in the real world, and an interaction method based on touch gestures is implemented. The entire method is independent of the network to ensure robustness to poor signal conditions. A prototype system was developed and tested on the Wuhan University campus to evaluate the method and validate its results. The findings demonstrate that our method achieves a high detection accuracy, stable geovisualization results and interaction.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">rao2017mobile</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A mobile outdoor augmented reality method combining deep learning object detection and spatial relationships for geovisualization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rao, Jinmeng and Qiao, Yanjun and Ren, Fu and Wang, Junxing and Du, Qingyun}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Sensors}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{17}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{9}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1951}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{MDPI}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2023 Jinmeng Rao. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?99f5dd6bf47fe01169a4267f279df989"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>